{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a987540",
   "metadata": {},
   "source": [
    "## Step 1: Fetching episode details for TN show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f909b61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99c7042e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spotify client ready.\n"
     ]
    }
   ],
   "source": [
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from spotipy import Spotify\n",
    "# Hide this if publishing code\n",
    "SPOTIFY_CLIENT_ID='9353a14355e84013a72bec5f76e8c65a'\n",
    "SPOTIFY_CLIENT_SECRET='bba51938dbb547f288ec5eb029c4fba8'\n",
    "\n",
    "if not SPOTIFY_CLIENT_ID or not SPOTIFY_CLIENT_SECRET:\n",
    "    raise RuntimeError(\"‚ö†Ô∏è  Missing Spotify API credentials. Set SPOTIFY_CLIENT_ID and SPOTIFY_CLIENT_SECRET.\")\n",
    "\n",
    "# Create the client\n",
    "auth = SpotifyClientCredentials(client_id=SPOTIFY_CLIENT_ID, client_secret=SPOTIFY_CLIENT_SECRET)\n",
    "sp = Spotify(client_credentials_manager=auth, requests_timeout=30, retries=3)\n",
    "print(\"‚úÖ Spotify client ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcc502ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Retrieved 95 episodes.\n"
     ]
    }
   ],
   "source": [
    "SHOW_ID = \"122imavATqSE7eCyXIcqZL\"   # Trevor Noah podcast ID\n",
    "MARKET = \"US\"                         # adjust if needed\n",
    "OUT_PATH = \"data/trevor_episodes.jsonl\"\n",
    "\n",
    "os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)\n",
    "\n",
    "def fetch_all_episodes(sp, show_id, market=\"US\"):\n",
    "    \"\"\"Paginate through all episodes of a Spotify show.\"\"\"\n",
    "    all_eps = []\n",
    "    limit, offset = 50, 0\n",
    "    while True:\n",
    "        results = sp.show_episodes(show_id, limit=limit, offset=offset, market=market)\n",
    "        eps = results.get(\"items\", [])\n",
    "        all_eps.extend(eps)\n",
    "        if not results.get(\"next\"):\n",
    "            break\n",
    "        offset += limit\n",
    "    return all_eps\n",
    "\n",
    "episodes = fetch_all_episodes(sp, SHOW_ID, MARKET)\n",
    "print(f\"‚úÖ Retrieved {len(episodes)} episodes.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78bb5f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved metadata for 95 episodes to data/trevor_episodes.jsonl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>release_date</th>\n",
       "      <th>name</th>\n",
       "      <th>description_snippet</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-10-21</td>\n",
       "      <td>99% Invisible and the Megachurch (featuring Gi...</td>\n",
       "      <td>Roman Mars and frequent 99% Invisible contribu...</td>\n",
       "      <td>https://open.spotify.com/episode/257sC2OwsIs9w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-10-16</td>\n",
       "      <td>The Real Malala: Jeans, Crushes, &amp; Healing</td>\n",
       "      <td>Malala like you‚Äôve never seen her -- In this d...</td>\n",
       "      <td>https://open.spotify.com/episode/1UXnBqGu6wZJP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-10-09</td>\n",
       "      <td>Who Owns America? Bernie Sanders Says the Quie...</td>\n",
       "      <td>Senator Bernie Sanders joins Trevor to discuss...</td>\n",
       "      <td>https://open.spotify.com/episode/4DFir6obQmJI3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>Kara Swisher: Tech, Power, and Why You Should ...</td>\n",
       "      <td>Trevor and Eugene Khoza have a wide-ranging co...</td>\n",
       "      <td>https://open.spotify.com/episode/47uapPMiGcjnq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-09-25</td>\n",
       "      <td>Ezra Klein: America At Its Breaking Point</td>\n",
       "      <td>New York Times journalist and political commen...</td>\n",
       "      <td>https://open.spotify.com/episode/0IhLeiYRrSFkF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-09-18</td>\n",
       "      <td>Will AI Save Humanity or End It? with Mustafa ...</td>\n",
       "      <td>Trevor (who is also Microsoft‚Äôs ‚ÄúChief Questio...</td>\n",
       "      <td>https://open.spotify.com/episode/3lHbHhoADv6cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-09-15</td>\n",
       "      <td>What Now Is Back!</td>\n",
       "      <td>What Now is back! Join Trevor each week for co...</td>\n",
       "      <td>https://open.spotify.com/episode/1QFuHR2EECai6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-08-28</td>\n",
       "      <td>Between the Seasons: Stories from a South Afri...</td>\n",
       "      <td>As we gear up for our new season launch on Sep...</td>\n",
       "      <td>https://open.spotify.com/episode/5nn997mIIyi6t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-07-10</td>\n",
       "      <td>Between the Seasons: Trump Dropping Bombs for ...</td>\n",
       "      <td>What Now is taking a summer break! Season 3 wi...</td>\n",
       "      <td>https://open.spotify.com/episode/6v5mAFeZHGESj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-06-26</td>\n",
       "      <td>The Totally Very Real White Genocide in South ...</td>\n",
       "      <td>Television and radio host Dan Corder and comed...</td>\n",
       "      <td>https://open.spotify.com/episode/0bMCELObBRi5C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  release_date                                               name  \\\n",
       "0   2025-10-21  99% Invisible and the Megachurch (featuring Gi...   \n",
       "1   2025-10-16         The Real Malala: Jeans, Crushes, & Healing   \n",
       "2   2025-10-09  Who Owns America? Bernie Sanders Says the Quie...   \n",
       "3   2025-10-02  Kara Swisher: Tech, Power, and Why You Should ...   \n",
       "4   2025-09-25          Ezra Klein: America At Its Breaking Point   \n",
       "5   2025-09-18  Will AI Save Humanity or End It? with Mustafa ...   \n",
       "6   2025-09-15                                  What Now Is Back!   \n",
       "7   2025-08-28  Between the Seasons: Stories from a South Afri...   \n",
       "8   2025-07-10  Between the Seasons: Trump Dropping Bombs for ...   \n",
       "9   2025-06-26  The Totally Very Real White Genocide in South ...   \n",
       "\n",
       "                                 description_snippet  \\\n",
       "0  Roman Mars and frequent 99% Invisible contribu...   \n",
       "1  Malala like you‚Äôve never seen her -- In this d...   \n",
       "2  Senator Bernie Sanders joins Trevor to discuss...   \n",
       "3  Trevor and Eugene Khoza have a wide-ranging co...   \n",
       "4  New York Times journalist and political commen...   \n",
       "5  Trevor (who is also Microsoft‚Äôs ‚ÄúChief Questio...   \n",
       "6  What Now is back! Join Trevor each week for co...   \n",
       "7  As we gear up for our new season launch on Sep...   \n",
       "8  What Now is taking a summer break! Season 3 wi...   \n",
       "9  Television and radio host Dan Corder and comed...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://open.spotify.com/episode/257sC2OwsIs9w...  \n",
       "1  https://open.spotify.com/episode/1UXnBqGu6wZJP...  \n",
       "2  https://open.spotify.com/episode/4DFir6obQmJI3...  \n",
       "3  https://open.spotify.com/episode/47uapPMiGcjnq...  \n",
       "4  https://open.spotify.com/episode/0IhLeiYRrSFkF...  \n",
       "5  https://open.spotify.com/episode/3lHbHhoADv6cr...  \n",
       "6  https://open.spotify.com/episode/1QFuHR2EECai6...  \n",
       "7  https://open.spotify.com/episode/5nn997mIIyi6t...  \n",
       "8  https://open.spotify.com/episode/6v5mAFeZHGESj...  \n",
       "9  https://open.spotify.com/episode/0bMCELObBRi5C...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = []\n",
    "with open(OUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    for ep in episodes:\n",
    "        rec = {\n",
    "            \"episode_id\": ep[\"id\"],\n",
    "            \"name\": ep.get(\"name\"),\n",
    "            \"description\": ep.get(\"description\"),\n",
    "            \"release_date\": ep.get(\"release_date\"),\n",
    "            \"duration_ms\": ep.get(\"duration_ms\"),\n",
    "            \"language\": ep.get(\"language\"),\n",
    "            \"url\": ep.get(\"external_urls\", {}).get(\"spotify\"),\n",
    "            \"show_id\": SHOW_ID\n",
    "        }\n",
    "        records.append(rec)\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"üíæ Saved metadata for {len(records)} episodes to {OUT_PATH}\")\n",
    "\n",
    "# Quick peek\n",
    "df = pd.DataFrame(records)\n",
    "df[\"description_snippet\"] = df[\"description\"].str.replace(\"\\n\",\" \").str.slice(0,200)\n",
    "df[[\"release_date\",\"name\",\"description_snippet\",\"url\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "275f55cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from typing import List, Optional\n",
    "\n",
    "# load spaCy model (small one is fine)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Regex-based candidate patterns\n",
    "# -------------------------------\n",
    "PATTERNS = [\n",
    "    r\"with\\s+([A-Z][\\w'.-]+(?:\\s+[A-Z][\\w'.-]+){0,3})\",     # e.g. \"with Barack Obama\"\n",
    "    r\"featuring\\s+([A-Z][\\w'.-]+(?:\\s+[A-Z][\\w'.-]+){0,3})\",\n",
    "    r\"meet\\s+([A-Z][\\w'.-]+(?:\\s+[A-Z][\\w'.-]+){0,3})\",\n",
    "    r\"guest[:\\s-]+([A-Z][\\w'.-]+(?:\\s+[A-Z][\\w'.-]+){0,3})\",\n",
    "    r\"joins\\s+([A-Z][\\w'.-]+(?:\\s+[A-Z][\\w'.-]+){0,3})\",\n",
    "    r\"joined by\\s+([A-Z][\\w'.-]+(?:\\s+[A-Z][\\w'.-]+){0,3})\",\n",
    "]\n",
    "\n",
    "BLACKLIST = {\"trevor\", \"noah\", \"episode\", \"season\", \"bonus\", \"trailer\"}\n",
    "MAX_TOKENS = 4  # max number of words in a name\n",
    "\n",
    "\n",
    "def rule_candidates(text: str) -> List[str]:\n",
    "    \"\"\"Return rule-based name candidates from the text.\"\"\"\n",
    "    cands = []\n",
    "    for pat in PATTERNS:\n",
    "        for m in re.finditer(pat, text, flags=re.IGNORECASE):\n",
    "            cands.append(m.group(1).strip())\n",
    "    return cands\n",
    "\n",
    "\n",
    "def ner_candidates(text: str) -> List[str]:\n",
    "    \"\"\"Return PERSON entities detected by spaCy.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [ent.text.strip() for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "\n",
    "\n",
    "def plausible(name: str) -> bool:\n",
    "    \"\"\"Basic sanity filter for name plausibility.\"\"\"\n",
    "    parts = name.split()\n",
    "    if not parts:\n",
    "        return False\n",
    "    if any(p.lower() in BLACKLIST for p in parts):\n",
    "        return False\n",
    "    if len(parts) > MAX_TOKENS:\n",
    "        return False\n",
    "    # ensure capitalization for at least the first word\n",
    "    if not parts[0][0].isupper():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def extract_guest_name(title: str) -> Optional[str]:\n",
    "    \"\"\"Extract the most likely guest name from an episode title.\"\"\"\n",
    "    if not isinstance(title, str) or not title.strip():\n",
    "        return None\n",
    "    text = title.strip()\n",
    "    # try rule-based first\n",
    "    for name in rule_candidates(text):\n",
    "        if plausible(name):\n",
    "            return name\n",
    "    # fallback: NER\n",
    "    for name in ner_candidates(text):\n",
    "        if plausible(name):\n",
    "            return name\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "192bd8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>release_date</th>\n",
       "      <th>name</th>\n",
       "      <th>guest_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-10-21</td>\n",
       "      <td>99% Invisible and the Megachurch (featuring Gi...</td>\n",
       "      <td>Gillian Jacobs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-10-16</td>\n",
       "      <td>The Real Malala: Jeans, Crushes, &amp; Healing</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-10-09</td>\n",
       "      <td>Who Owns America? Bernie Sanders Says the Quie...</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>Kara Swisher: Tech, Power, and Why You Should ...</td>\n",
       "      <td>Kara Swisher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-09-25</td>\n",
       "      <td>Ezra Klein: America At Its Breaking Point</td>\n",
       "      <td>Ezra Klein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-09-18</td>\n",
       "      <td>Will AI Save Humanity or End It? with Mustafa ...</td>\n",
       "      <td>Mustafa Suleyman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-09-15</td>\n",
       "      <td>What Now Is Back!</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-08-28</td>\n",
       "      <td>Between the Seasons: Stories from a South Afri...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-07-10</td>\n",
       "      <td>Between the Seasons: Trump Dropping Bombs for ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-06-26</td>\n",
       "      <td>The Totally Very Real White Genocide in South ...</td>\n",
       "      <td>Dan Corder and Eugene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-06-19</td>\n",
       "      <td>If I Ruled the World: Creating Chaos with Anel...</td>\n",
       "      <td>Anele and Sizwe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-06-12</td>\n",
       "      <td>Meet Jon Stewart ‚Äì One of My Favorite People</td>\n",
       "      <td>Jon Stewart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>If I Ruled the World: Tressie McMillan Cottom ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-05-29</td>\n",
       "      <td>Meet Esther Perel ‚Äì One of My Favorite People</td>\n",
       "      <td>Esther Perel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2025-05-22</td>\n",
       "      <td>The Perfect Algorithm for Finding Love? with C...</td>\n",
       "      <td>Christian Rudder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2025-05-15</td>\n",
       "      <td>The Anxious Generation with Jonathan Haidt</td>\n",
       "      <td>Jonathan Haidt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2025-05-08</td>\n",
       "      <td>Meet Derek Fordjour ‚Äì One of My Favorite People</td>\n",
       "      <td>Derek Fordjour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>Neuro-Spicy: Discussing ADHD with Dr. Kristin ...</td>\n",
       "      <td>Dr. Kristin Carothers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2025-04-24</td>\n",
       "      <td>If I Ruled the World: Careful What You Think</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2025-04-17</td>\n",
       "      <td>Meet Eugene Khoza ‚Äì One of My Favorite People</td>\n",
       "      <td>Eugene Khoza</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   release_date                                               name  \\\n",
       "0    2025-10-21  99% Invisible and the Megachurch (featuring Gi...   \n",
       "1    2025-10-16         The Real Malala: Jeans, Crushes, & Healing   \n",
       "2    2025-10-09  Who Owns America? Bernie Sanders Says the Quie...   \n",
       "3    2025-10-02  Kara Swisher: Tech, Power, and Why You Should ...   \n",
       "4    2025-09-25          Ezra Klein: America At Its Breaking Point   \n",
       "5    2025-09-18  Will AI Save Humanity or End It? with Mustafa ...   \n",
       "6    2025-09-15                                  What Now Is Back!   \n",
       "7    2025-08-28  Between the Seasons: Stories from a South Afri...   \n",
       "8    2025-07-10  Between the Seasons: Trump Dropping Bombs for ...   \n",
       "9    2025-06-26  The Totally Very Real White Genocide in South ...   \n",
       "10   2025-06-19  If I Ruled the World: Creating Chaos with Anel...   \n",
       "11   2025-06-12       Meet Jon Stewart ‚Äì One of My Favorite People   \n",
       "12   2025-06-05  If I Ruled the World: Tressie McMillan Cottom ...   \n",
       "13   2025-05-29      Meet Esther Perel ‚Äì One of My Favorite People   \n",
       "14   2025-05-22  The Perfect Algorithm for Finding Love? with C...   \n",
       "15   2025-05-15         The Anxious Generation with Jonathan Haidt   \n",
       "16   2025-05-08    Meet Derek Fordjour ‚Äì One of My Favorite People   \n",
       "17   2025-05-01  Neuro-Spicy: Discussing ADHD with Dr. Kristin ...   \n",
       "18   2025-04-24       If I Ruled the World: Careful What You Think   \n",
       "19   2025-04-17      Meet Eugene Khoza ‚Äì One of My Favorite People   \n",
       "\n",
       "               guest_name  \n",
       "0          Gillian Jacobs  \n",
       "1                    None  \n",
       "2          Bernie Sanders  \n",
       "3            Kara Swisher  \n",
       "4              Ezra Klein  \n",
       "5        Mustafa Suleyman  \n",
       "6                    None  \n",
       "7                    None  \n",
       "8                    None  \n",
       "9   Dan Corder and Eugene  \n",
       "10        Anele and Sizwe  \n",
       "11            Jon Stewart  \n",
       "12                   None  \n",
       "13           Esther Perel  \n",
       "14       Christian Rudder  \n",
       "15         Jonathan Haidt  \n",
       "16         Derek Fordjour  \n",
       "17  Dr. Kristin Carothers  \n",
       "18                   None  \n",
       "19           Eugene Khoza  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 58 episodes with apparent guests out of 95 total.\n"
     ]
    }
   ],
   "source": [
    "df[\"guest_name\"] = df[\"name\"].apply(extract_guest_name)\n",
    "\n",
    "# Inspect results\n",
    "df_result = df[[\"release_date\", \"name\", \"guest_name\"]].copy()\n",
    "display(df_result.head(20))\n",
    "\n",
    "# How many episodes have a detected guest?\n",
    "num_with_guests = df_result[\"guest_name\"].notna().sum()\n",
    "print(f\"‚úÖ Found {num_with_guests} episodes with apparent guests out of {len(df_result)} total.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98d6cc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Manual guest names inserted.\n"
     ]
    }
   ],
   "source": [
    "# manually correct guest names that were missed\n",
    "manual_updates = {\n",
    "    1:  \"Malala Yousafzai\",\n",
    "    12: \"Tressie McMillan Cottom\",\n",
    "    21: \"Scott Galloway\",\n",
    "    27: \"Halle Berry\",\n",
    "    29: \"Ben Winston\",\n",
    "    33: \"Marques Brownlee\",\n",
    "    46: \"Yuval Noah Harari\",\n",
    "    53: \"Adam Grant\",\n",
    "    57: \"Jody Avirgan\",\n",
    "    58: \"Questlove\",\n",
    "    61: \"Julia Louis-Dreyfus\",\n",
    "    63: \"Jerrod Charmichael\",\n",
    "    64: \"Brad Smith\",\n",
    "    70: \"Orlando Bloom\",\n",
    "    90: \"DaBaby\"\n",
    "}\n",
    "\n",
    "for idx, name in manual_updates.items():\n",
    "    if idx < len(df):\n",
    "        df.at[idx, \"guest_name\"] = name\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Skipped index {idx} ‚Äî out of range (DataFrame has {len(df)} rows).\")\n",
    "\n",
    "print(\"‚úÖ Manual guest names inserted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07f349f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export df name, guest_name and description snippet to csv\n",
    "df_export = df[[\"name\", \"guest_name\", \"description_snippet\"]]\n",
    "df_export.to_csv(\"data/trevor_guest_names.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd78c39",
   "metadata": {},
   "source": [
    "## Searching guest names in Search Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "311b6463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export test file\n",
    "# import csv trevor_guest_names.csv\n",
    "df = pd.read_csv(\"data/trevor_guest_names_fixed.csv\")\n",
    "\n",
    "\n",
    "#df_test = df[:3]       \n",
    "#df_test.to_csv(\"data/trevor_guest_names_test.csv\", index=False)\n",
    "\n",
    "df_subset =df[df['changed']==1]\n",
    "\n",
    "df_subset.to_csv(\"data/trevor_guest_names_updates.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42f6333",
   "metadata": {},
   "source": [
    "# Starting new file to not break whole flow if I have to rerun pipeline (24-10-2025) - see v2 file for flow with just updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6924e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Spotify for 65 unique guest names...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "guests:  12%|‚ñà‚ñè        | 8/65 [01:35<07:06,  7.48s/it]WARNING:root:Your application has reached a rate/request limit. Retry will occur after: None\n",
      "guests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 65/65 [12:10<00:00, 11.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Wrote 32424 matching records to data/guest_episode_matches.jsonl\n",
      "Wrote summary per-guest to data/guest_episode_summary.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# SEARCH SPOTIFY FOR EPISODES MATCHING GUEST NAMES\n",
    "# ------------------------------------------------\n",
    "# Requires:\n",
    "#  - 'sp' in scope: a spotipy.Spotify client created via client credentials\n",
    "#  - input CSV: /mnt/data/trevor_guest_names.csv with column 'guest_name'\n",
    "# Outputs:\n",
    "#  - data/guest_episode_matches.jsonl  (one JSON per match)\n",
    "#  - data/guest_episode_summary.csv   (summary per guest)\n",
    "\n",
    "import json, os, time, math\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from spotipy.exceptions import SpotifyException\n",
    "\n",
    "INPUT = \"data/trevor_guest_names.csv\"\n",
    "OUT_JSONL = \"data/guest_episode_matches.jsonl\"\n",
    "OUT_SUMMARY = \"data/guest_episode_summary.csv\"\n",
    "\n",
    "# CONFIG\n",
    "LIMIT_PER_REQUEST = 50        # max allowed by Spotify per docs (range 0-50)\n",
    "MAX_OFFSET = 1000             # Spotify docs limit on offset (range 0-1000)\n",
    "MAX_PER_GUEST = None          # set to None to fetch all pages up to offset cap; or set e.g. 500\n",
    "SLEEP_BETWEEN_REQUESTS = 0.3  # polite pause\n",
    "\n",
    "# helper: safe search with backoff for 429\n",
    "def spotify_search_episodes(sp, query, market=\"US\", limit=50, offset=0, max_retries=5):\n",
    "    backoff = 1.0\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            res = sp.search(q=query, type=\"episode\", market=market, limit=limit, offset=offset)\n",
    "            return res\n",
    "        except SpotifyException as e:\n",
    "            status = getattr(e, \"http_status\", None)\n",
    "            # spotipy raises SpotifyException; check message for 429\n",
    "            if status == 429 or (\"429\" in str(e)):\n",
    "                # extract Retry-After header if available\n",
    "                retry_after = None\n",
    "                try:\n",
    "                    retry_after = int(e.headers.get(\"Retry-After\"))\n",
    "                except Exception:\n",
    "                    retry_after = None\n",
    "                wait = retry_after if retry_after is not None else backoff\n",
    "                print(f\"Rate limited (429). Waiting {wait} seconds before retrying (attempt {attempt+1}/{max_retries})...\")\n",
    "                time.sleep(wait)\n",
    "                backoff *= 2\n",
    "                continue\n",
    "            else:\n",
    "                # re-raise for other problems\n",
    "                raise\n",
    "    raise RuntimeError(\"Max retries exceeded for spotify_search_episodes\")\n",
    "\n",
    "# load guests\n",
    "df_guests = pd.read_csv(INPUT)\n",
    "if \"guest_name\" not in df_guests.columns:\n",
    "    raise RuntimeError(\"CSV must contain 'guest_name' column\")\n",
    "\n",
    "unique_guests = df_guests[\"guest_name\"].dropna().astype(str).map(str.strip).unique().tolist()\n",
    "print(f\"Searching Spotify for {len(unique_guests)} unique guest names...\")\n",
    "\n",
    "os.makedirs(os.path.dirname(OUT_JSONL), exist_ok=True)\n",
    "\n",
    "# We'll write matches incrementally\n",
    "written = 0\n",
    "summary_rows = []\n",
    "\n",
    "for guest in tqdm(unique_guests, desc=\"guests\"):\n",
    "    guest_matches = []\n",
    "    # Build queries - try exact phrase first (quotes) then fallback\n",
    "    queries = [f'\"{guest}\"']  \n",
    "    # keep a set of seen episode ids to avoid duplicates across queries\n",
    "    seen_episode_ids = set()\n",
    "    # Determine how many items we'll attempt to fetch: if MAX_PER_GUEST set, cap pages\n",
    "    items_to_fetch = MAX_PER_GUEST if MAX_PER_GUEST is not None else float(\"inf\")\n",
    "    fetched_count = 0\n",
    "\n",
    "    for q in queries:\n",
    "        offset = 0\n",
    "        while True:\n",
    "            # Spotify's offset must not exceed 1000 per docs\n",
    "            if offset > MAX_OFFSET:\n",
    "                print(f\"Reached Spotify offset cap for guest {guest}. Stopping pagination at offset {offset}.\")\n",
    "                break\n",
    "\n",
    "            # determine how many to request in this page\n",
    "            limit = LIMIT_PER_REQUEST\n",
    "            # if user set MAX_PER_GUEST, do not exceed that\n",
    "            if items_to_fetch != float(\"inf\"):\n",
    "                remaining = items_to_fetch - fetched_count\n",
    "                if remaining <= 0:\n",
    "                    break\n",
    "                limit = min(limit, max(1, int(remaining)))\n",
    "\n",
    "            # call Spotify\n",
    "            res = spotify_search_episodes(sp, q, market=\"US\", limit=limit, offset=offset)\n",
    "            eps_block = res.get(\"episodes\", {}).get(\"items\", [])\n",
    "            total_available = res.get(\"episodes\", {}).get(\"total\", None)\n",
    "\n",
    "            # process items\n",
    "            for ep in eps_block:\n",
    "                eid = ep.get(\"id\")\n",
    "                if not eid or eid in seen_episode_ids:\n",
    "                    continue\n",
    "                seen_episode_ids.add(eid)\n",
    "                rec = {\n",
    "                    \"guest\": guest,\n",
    "                    \"query\": q,\n",
    "                    \"episode_id\": ep.get(\"id\"),\n",
    "                    \"episode_name\": ep.get(\"name\"),\n",
    "                    \"episode_description\": ep.get(\"description\"),\n",
    "                    \"episode_url\": ep.get(\"external_urls\", {}).get(\"spotify\"),\n",
    "                    \"release_date\": ep.get(\"release_date\"),\n",
    "                    \"show_id\": (ep.get(\"show\") or {}).get(\"id\"),\n",
    "                    \"show_name\": (ep.get(\"show\") or {}).get(\"name\"),\n",
    "                    \"show_publisher\": (ep.get(\"show\") or {}).get(\"publisher\"),\n",
    "                    \"fetched_at\": time.time()\n",
    "                }\n",
    "                # append to file\n",
    "                with open(OUT_JSONL, \"a\", encoding=\"utf-8\") as fout:\n",
    "                    fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "                guest_matches.append(rec)\n",
    "                written += 1\n",
    "                fetched_count += 1\n",
    "\n",
    "            # progress & paging decision\n",
    "            # If no 'next' (or fewer items than limit), stop paging this query\n",
    "            next_url = res.get(\"episodes\", {}).get(\"next\")\n",
    "            if not next_url:\n",
    "                break\n",
    "\n",
    "            # Next offset\n",
    "            offset += limit\n",
    "\n",
    "            # Keep polite\n",
    "            time.sleep(SLEEP_BETWEEN_REQUESTS)\n",
    "\n",
    "            # If we've hit configured MAX_PER_GUEST, stop\n",
    "            if items_to_fetch != float(\"inf\") and fetched_count >= items_to_fetch:\n",
    "                break\n",
    "\n",
    "        # if we've found some matches with exact-phrase, you might skip loose query to avoid duplicates;\n",
    "        # decide policy here ‚Äî currently we will still run the fallback (it dedupes by episode id).\n",
    "    summary_rows.append({\n",
    "        \"guest\": guest,\n",
    "        \"matches_found\": len(guest_matches),\n",
    "        \"unique_episode_ids\": len(seen_episode_ids),\n",
    "        \"sample_episode_id\": (next(iter(seen_episode_ids)) if len(seen_episode_ids)>0 else None),\n",
    "        \"total_available_reported\": total_available\n",
    "    })\n",
    "\n",
    "print(f\"Done. Wrote {written} matching records to {OUT_JSONL}\")\n",
    "\n",
    "# write summary CSV\n",
    "pd.DataFrame(summary_rows).to_csv(OUT_SUMMARY, index=False)\n",
    "print(f\"Wrote summary per-guest to {OUT_SUMMARY}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9de1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 66428 matching records.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guest</th>\n",
       "      <th>query</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>episode_description</th>\n",
       "      <th>episode_url</th>\n",
       "      <th>release_date</th>\n",
       "      <th>show_id</th>\n",
       "      <th>show_name</th>\n",
       "      <th>show_publisher</th>\n",
       "      <th>fetched_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gillian Jacobs</td>\n",
       "      <td>\"Gillian Jacobs\"</td>\n",
       "      <td>1IXqOEvAnjpvKdwE5zWoYl</td>\n",
       "      <td>Mr. Yuk</td>\n",
       "      <td>Mr. Yuk is a neon green circular sticker with ...</td>\n",
       "      <td>https://open.spotify.com/episode/1IXqOEvAnjpvK...</td>\n",
       "      <td>2024-04-30</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.761057e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gillian Jacobs</td>\n",
       "      <td>\"Gillian Jacobs\"</td>\n",
       "      <td>7FOKIo7hxiwxMBq6W4blwu</td>\n",
       "      <td>Don‚Äôt Call Me Daddy (Kerri Kenney-Silver, Dan ...</td>\n",
       "      <td>This week, Scott is joined by bucket list gues...</td>\n",
       "      <td>https://open.spotify.com/episode/7FOKIo7hxiwxM...</td>\n",
       "      <td>2025-06-30</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.761057e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gillian Jacobs</td>\n",
       "      <td>\"Gillian Jacobs\"</td>\n",
       "      <td>3k5FkGBErXRX68xp0f8Nro</td>\n",
       "      <td>Bonus Bang: Gillian Jacobs, Paul F. Tompkins, ...</td>\n",
       "      <td>This is episode 3 in our \"More-imony Tony\" ser...</td>\n",
       "      <td>https://open.spotify.com/episode/3k5FkGBErXRX6...</td>\n",
       "      <td>2025-01-23</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.761057e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gillian Jacobs</td>\n",
       "      <td>\"Gillian Jacobs\"</td>\n",
       "      <td>3uhR3UNbKWmN7Uhd17q00x</td>\n",
       "      <td>Taco Bell 4 with Gillian Jacobs</td>\n",
       "      <td>Actress Gillian Jacobs (Love, Life of the Part...</td>\n",
       "      <td>https://open.spotify.com/episode/3uhR3UNbKWmN7...</td>\n",
       "      <td>2018-05-31</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.761057e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gillian Jacobs</td>\n",
       "      <td>\"Gillian Jacobs\"</td>\n",
       "      <td>0E7S5KF0zmDrVdwSpVxBYe</td>\n",
       "      <td>Bonus Bang: Paul Rust, Gillian Jacobs, Paul F....</td>\n",
       "      <td>This is episode 5 of our \"Old No-Nos\" series, ...</td>\n",
       "      <td>https://open.spotify.com/episode/0E7S5KF0zmDrV...</td>\n",
       "      <td>2024-12-05</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.761057e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gillian Jacobs</td>\n",
       "      <td>\"Gillian Jacobs\"</td>\n",
       "      <td>4LFxdcRqtI2kuL26tvDS58</td>\n",
       "      <td>Bonus Bang: Paul Rust, Gillian Jacobs, Paul F....</td>\n",
       "      <td>This is episode 4 of our \"Old No-No‚Äôs‚Äù series,...</td>\n",
       "      <td>https://open.spotify.com/episode/4LFxdcRqtI2ku...</td>\n",
       "      <td>2024-11-28</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.761057e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gillian Jacobs</td>\n",
       "      <td>\"Gillian Jacobs\"</td>\n",
       "      <td>5cAr9VuLaZ9MdqymVTQPHH</td>\n",
       "      <td>Gillian Jacobs loves dinosaurs</td>\n",
       "      <td>My guest this week is actor Gillian Jacobs.Gil...</td>\n",
       "      <td>https://open.spotify.com/episode/5cAr9VuLaZ9Md...</td>\n",
       "      <td>2023-09-18</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.761057e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Gillian Jacobs</td>\n",
       "      <td>\"Gillian Jacobs\"</td>\n",
       "      <td>3rNGCXnfY3pNR7uxCMnmYT</td>\n",
       "      <td>Gillian Jacobs: Bad Landlord Stories (The Andy...</td>\n",
       "      <td>Actress Gillian Jacobs (Community, The Bear) j...</td>\n",
       "      <td>https://open.spotify.com/episode/3rNGCXnfY3pNR...</td>\n",
       "      <td>2025-05-02</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.761057e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Gillian Jacobs</td>\n",
       "      <td>\"Gillian Jacobs\"</td>\n",
       "      <td>30XXMt9SipT4qgmXsu7gZt</td>\n",
       "      <td>99% Invisible and the Megachurch (featuring Gi...</td>\n",
       "      <td>Roman Mars and frequent 99% Invisible contribu...</td>\n",
       "      <td>https://open.spotify.com/episode/30XXMt9SipT4q...</td>\n",
       "      <td>2025-10-21</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.761057e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Gillian Jacobs</td>\n",
       "      <td>\"Gillian Jacobs\"</td>\n",
       "      <td>0VJp3uMwOeGaalB8TC6jYS</td>\n",
       "      <td>Gillian Jacobs talks Mimi-Rose Howard | HBO's ...</td>\n",
       "      <td>This week on Girls Rewatch Podcast, we‚Äôre hitt...</td>\n",
       "      <td>https://open.spotify.com/episode/0VJp3uMwOeGaa...</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.761057e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            guest             query              episode_id  \\\n",
       "0  Gillian Jacobs  \"Gillian Jacobs\"  1IXqOEvAnjpvKdwE5zWoYl   \n",
       "1  Gillian Jacobs  \"Gillian Jacobs\"  7FOKIo7hxiwxMBq6W4blwu   \n",
       "2  Gillian Jacobs  \"Gillian Jacobs\"  3k5FkGBErXRX68xp0f8Nro   \n",
       "3  Gillian Jacobs  \"Gillian Jacobs\"  3uhR3UNbKWmN7Uhd17q00x   \n",
       "4  Gillian Jacobs  \"Gillian Jacobs\"  0E7S5KF0zmDrVdwSpVxBYe   \n",
       "5  Gillian Jacobs  \"Gillian Jacobs\"  4LFxdcRqtI2kuL26tvDS58   \n",
       "6  Gillian Jacobs  \"Gillian Jacobs\"  5cAr9VuLaZ9MdqymVTQPHH   \n",
       "7  Gillian Jacobs  \"Gillian Jacobs\"  3rNGCXnfY3pNR7uxCMnmYT   \n",
       "8  Gillian Jacobs  \"Gillian Jacobs\"  30XXMt9SipT4qgmXsu7gZt   \n",
       "9  Gillian Jacobs  \"Gillian Jacobs\"  0VJp3uMwOeGaalB8TC6jYS   \n",
       "\n",
       "                                        episode_name  \\\n",
       "0                                            Mr. Yuk   \n",
       "1  Don‚Äôt Call Me Daddy (Kerri Kenney-Silver, Dan ...   \n",
       "2  Bonus Bang: Gillian Jacobs, Paul F. Tompkins, ...   \n",
       "3                    Taco Bell 4 with Gillian Jacobs   \n",
       "4  Bonus Bang: Paul Rust, Gillian Jacobs, Paul F....   \n",
       "5  Bonus Bang: Paul Rust, Gillian Jacobs, Paul F....   \n",
       "6                     Gillian Jacobs loves dinosaurs   \n",
       "7  Gillian Jacobs: Bad Landlord Stories (The Andy...   \n",
       "8  99% Invisible and the Megachurch (featuring Gi...   \n",
       "9  Gillian Jacobs talks Mimi-Rose Howard | HBO's ...   \n",
       "\n",
       "                                 episode_description  \\\n",
       "0  Mr. Yuk is a neon green circular sticker with ...   \n",
       "1  This week, Scott is joined by bucket list gues...   \n",
       "2  This is episode 3 in our \"More-imony Tony\" ser...   \n",
       "3  Actress Gillian Jacobs (Love, Life of the Part...   \n",
       "4  This is episode 5 of our \"Old No-Nos\" series, ...   \n",
       "5  This is episode 4 of our \"Old No-No‚Äôs‚Äù series,...   \n",
       "6  My guest this week is actor Gillian Jacobs.Gil...   \n",
       "7  Actress Gillian Jacobs (Community, The Bear) j...   \n",
       "8  Roman Mars and frequent 99% Invisible contribu...   \n",
       "9  This week on Girls Rewatch Podcast, we‚Äôre hitt...   \n",
       "\n",
       "                                         episode_url release_date show_id  \\\n",
       "0  https://open.spotify.com/episode/1IXqOEvAnjpvK...   2024-04-30    None   \n",
       "1  https://open.spotify.com/episode/7FOKIo7hxiwxM...   2025-06-30    None   \n",
       "2  https://open.spotify.com/episode/3k5FkGBErXRX6...   2025-01-23    None   \n",
       "3  https://open.spotify.com/episode/3uhR3UNbKWmN7...   2018-05-31    None   \n",
       "4  https://open.spotify.com/episode/0E7S5KF0zmDrV...   2024-12-05    None   \n",
       "5  https://open.spotify.com/episode/4LFxdcRqtI2ku...   2024-11-28    None   \n",
       "6  https://open.spotify.com/episode/5cAr9VuLaZ9Md...   2023-09-18    None   \n",
       "7  https://open.spotify.com/episode/3rNGCXnfY3pNR...   2025-05-02    None   \n",
       "8  https://open.spotify.com/episode/30XXMt9SipT4q...   2025-10-21    None   \n",
       "9  https://open.spotify.com/episode/0VJp3uMwOeGaa...   2025-04-01    None   \n",
       "\n",
       "  show_name show_publisher    fetched_at  \n",
       "0      None           None  1.761057e+09  \n",
       "1      None           None  1.761057e+09  \n",
       "2      None           None  1.761057e+09  \n",
       "3      None           None  1.761057e+09  \n",
       "4      None           None  1.761057e+09  \n",
       "5      None           None  1.761057e+09  \n",
       "6      None           None  1.761057e+09  \n",
       "7      None           None  1.761057e+09  \n",
       "8      None           None  1.761057e+09  \n",
       "9      None           None  1.761057e+09  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn guest_episode_matches.jsonl into a DataFrame for analysis\n",
    "records = []\n",
    "with open(\"data/guest_episode_matches.jsonl\", \"r\", encoding=\"utf-8  \") as f:\n",
    "    for line in f:\n",
    "        rec = json.loads(line)\n",
    "        records.append(rec)\n",
    "df_matches = pd.DataFrame(records)\n",
    "print(f\"Loaded {len(df_matches)} matching records.\")\n",
    "df_matches.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2beb8690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32840\n"
     ]
    }
   ],
   "source": [
    "# Limit to only where guest name appears in episode_name\n",
    "\n",
    "df_matches_filtered = df_matches[df_matches.apply(lambda row: row['guest'].lower() in str(row['episode_name']).lower(), axis=1)]\n",
    "print(len(df_matches_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c64d899",
   "metadata": {},
   "source": [
    "## Retrieving Podcast ID from Get Episode endpoint and episode ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd97f404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, time, random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from spotipy.exceptions import SpotifyException\n",
    "\n",
    "# ---- Config ----\n",
    "MARKET = \"US\"\n",
    "BATCH_SIZE = 50                      # Spotify max for episodes endpoint\n",
    "CHECKPOINT_DIR = \"data\"\n",
    "CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, \"df_matches_showinfo_checkpoint.csv\")\n",
    "CACHE_PATH = os.path.join(CHECKPOINT_DIR, \"episode_show_cache.csv\")\n",
    "SAVE_EVERY = 1                       # save after every batch; increase if you want fewer writes\n",
    "MAX_RETRIES = 6\n",
    "BACKOFF_START = 1.0                  # seconds, exponential backoff base\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Ensure columns exist on df_matches_filtered\n",
    "for col in [\"show_id\", \"show_name\", \"show_publisher\"]:\n",
    "    if col not in df_matches_filtered.columns:\n",
    "        df_matches_filtered[col] = None\n",
    "\n",
    "# 1) Determine which episode_ids actually need fetching (de-dupe!)\n",
    "need_mask = df_matches_filtered[\"show_id\"].isna() | df_matches_filtered[\"show_name\"].isna()\n",
    "to_fetch_df = df_matches_filtered.loc[need_mask, [\"episode_id\"]].dropna().drop_duplicates()\n",
    "episode_ids_needed = to_fetch_df[\"episode_id\"].astype(str).tolist()\n",
    "print(f\"Unique episodes needing show info: {len(episode_ids_needed)}\")\n",
    "\n",
    "# 2) Load persistent cache (episode_id -> show fields)\n",
    "if os.path.exists(CACHE_PATH):\n",
    "    cache_df = pd.read_csv(CACHE_PATH, dtype=str)\n",
    "    cache_df = cache_df.dropna(subset=[\"episode_id\"]).drop_duplicates(\"episode_id\")\n",
    "    cache = {row[\"episode_id\"]: {\"show_id\": row.get(\"show_id\"),\n",
    "                                 \"show_name\": row.get(\"show_name\"),\n",
    "                                 \"show_publisher\": row.get(\"show_publisher\")} \n",
    "             for _, row in cache_df.iterrows()}\n",
    "else:\n",
    "    cache_df = pd.DataFrame(columns=[\"episode_id\",\"show_id\",\"show_name\",\"show_publisher\"])\n",
    "    cache = {}\n",
    "\n",
    "# 3) Drop ids already in cache\n",
    "episode_ids_needed = [eid for eid in episode_ids_needed if eid not in cache]\n",
    "print(f\"After cache, still need: {len(episode_ids_needed)}\")\n",
    "\n",
    "def save_checkpoint():\n",
    "    # merge cache into df and write both df + cache to disk\n",
    "    if cache:\n",
    "        new_rows = pd.DataFrame(\n",
    "            [{\"episode_id\": eid,\n",
    "              \"show_id\": v.get(\"show_id\"),\n",
    "              \"show_name\": v.get(\"show_name\"),\n",
    "              \"show_publisher\": v.get(\"show_publisher\")} \n",
    "             for eid, v in cache.items()]\n",
    "        )\n",
    "        # de-dupe on write\n",
    "        merged_cache = pd.concat([cache_df, new_rows], ignore_index=True)\n",
    "        merged_cache = merged_cache.dropna(subset=[\"episode_id\"]).drop_duplicates(\"episode_id\", keep=\"last\")\n",
    "        merged_cache.to_csv(CACHE_PATH, index=False)\n",
    "\n",
    "    # map cache into df_matches_filtered only for rows that still need it\n",
    "    if cache:\n",
    "        map_show_id = {k: v.get(\"show_id\") for k, v in cache.items()}\n",
    "        map_show_name = {k: v.get(\"show_name\") for k, v in cache.items()}\n",
    "        map_show_publisher = {k: v.get(\"show_publisher\") for k, v in cache.items()}\n",
    "        need_rows = df_matches_filtered[\"episode_id\"].astype(str).isin(cache.keys())\n",
    "        df_matches_filtered.loc[need_rows, \"show_id\"] = df_matches_filtered.loc[need_rows, \"episode_id\"].astype(str).map(map_show_id)\n",
    "        df_matches_filtered.loc[need_rows, \"show_name\"] = df_matches_filtered.loc[need_rows, \"episode_id\"].astype(str).map(map_show_name)\n",
    "        df_matches_filtered.loc[need_rows, \"show_publisher\"] = df_matches_filtered.loc[need_rows, \"episode_id\"].astype(str).map(map_show_publisher)\n",
    "\n",
    "    df_matches_filtered.to_csv(CHECKPOINT_PATH, index=False)\n",
    "    print(f\"üíæ Checkpoint saved ‚Üí {CHECKPOINT_PATH} | Cache ‚Üí {CACHE_PATH}\")\n",
    "\n",
    "def fetch_batch_with_backoff(sp, ids_batch, market=MARKET, max_retries=MAX_RETRIES):\n",
    "    \"\"\"Call sp.episodes(ids=...) with retry on 429 using Retry-After when available.\"\"\"\n",
    "    backoff = BACKOFF_START\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            res = sp.episodes(ids_batch, market=market)  # returns dict with \"episodes\": [ ... or None ... ]\n",
    "            return res.get(\"episodes\", [])\n",
    "        except SpotifyException as e:\n",
    "            status = getattr(e, \"http_status\", None)\n",
    "            if status == 429 or \"429\" in str(e):\n",
    "                # Respect Retry-After if provided\n",
    "                retry_after = None\n",
    "                try:\n",
    "                    retry_after = int(getattr(e, \"headers\", {}).get(\"Retry-After\"))  # spotipy sets headers on exception\n",
    "                except Exception:\n",
    "                    retry_after = None\n",
    "                wait = retry_after if retry_after is not None else backoff\n",
    "                wait += random.uniform(0, 0.25 * wait)  # jitter\n",
    "                print(f\"429 rate limit. Waiting {wait:.2f}s (attempt {attempt}/{max_retries}) ...\")\n",
    "                time.sleep(wait)\n",
    "                backoff = min(backoff * 2, 60)  # cap backoff\n",
    "                continue\n",
    "            # other transient statuses can also be retried\n",
    "            if status in (500, 502, 503, 504):\n",
    "                wait = backoff + random.uniform(0, 0.25 * backoff)\n",
    "                print(f\"{status} server error. Waiting {wait:.2f}s (attempt {attempt}/{max_retries}) ...\")\n",
    "                time.sleep(wait)\n",
    "                backoff = min(backoff * 2, 60)\n",
    "                continue\n",
    "            # non-retryable\n",
    "            print(f\"Non-retryable error {status}: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            # network hiccup retry\n",
    "            wait = backoff + random.uniform(0, 0.25 * backoff)\n",
    "            print(f\"Transient error: {e}. Waiting {wait:.2f}s (attempt {attempt}/{max_retries}) ...\")\n",
    "            time.sleep(wait)\n",
    "            backoff = min(backoff * 2, 60)\n",
    "    raise RuntimeError(\"Max retries exceeded for episodes batch\")\n",
    "\n",
    "# 4) Process in batches of 50, saving each time\n",
    "batches = [episode_ids_needed[i:i+BATCH_SIZE] for i in range(0, len(episode_ids_needed), BATCH_SIZE)]\n",
    "print(f\"Processing {len(batches)} batches of up to {BATCH_SIZE} episodes each.\")\n",
    "\n",
    "processed_batches = 0\n",
    "try:\n",
    "    for ids_batch in tqdm(batches):\n",
    "        # fetch a batch\n",
    "        episodes = fetch_batch_with_backoff(sp, ids_batch, market=MARKET)\n",
    "\n",
    "        # episodes list length == batch length typically; items may be None if invalid\n",
    "        for ep in episodes:\n",
    "            if not ep:\n",
    "                continue\n",
    "            eid = str(ep.get(\"id\"))\n",
    "            show = ep.get(\"show\") or {}\n",
    "            cache[eid] = {\n",
    "                \"show_id\": show.get(\"id\"),\n",
    "                \"show_name\": show.get(\"name\"),\n",
    "                \"show_publisher\": show.get(\"publisher\")\n",
    "            }\n",
    "\n",
    "        processed_batches += 1\n",
    "        if processed_batches % SAVE_EVERY == 0:\n",
    "            save_checkpoint()\n",
    "\n",
    "        # small polite delay between batches (optional; keep tiny since you now batch)\n",
    "        time.sleep(0.2 + random.uniform(0, 0.2))\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"‚èπÔ∏è Interrupted. Saving checkpoint...\")\n",
    "    save_checkpoint()\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"üí• Error: {e}\\nSaving checkpoint...\")\n",
    "    save_checkpoint()\n",
    "    raise\n",
    "else:\n",
    "    save_checkpoint()\n",
    "    print(\"‚úÖ Done. Filled show fields where available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cccbcedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to data/df_matches_with_showinfo_231025.csv\n"
     ]
    }
   ],
   "source": [
    "# Saving files\n",
    "df_matches_filtered.to_csv(\"data/df_matches_with_showinfo_231025.csv\", index=False)\n",
    "print(\"Saved to data/df_matches_with_showinfo_231025.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941712c8",
   "metadata": {},
   "source": [
    "## Clean up to only include shows where guests have overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d67d81e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches_filtered_loaded = pd.read_csv(\"data/df_matches_with_showinfo_231025.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "247cf669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30570\n"
     ]
    }
   ],
   "source": [
    "# Filtering incorrect guest names\n",
    "\n",
    "# Export list of unique guest names - first dedupe and then export\n",
    "df_matches_filtered_loaded.guest.drop_duplicates().to_csv(\"data/unique_guests.csv\", index=False)\n",
    "\n",
    "# Rows to remove if guest name contains: 'Dan Corder and Eugene', 'Anele and Sizwe', 'Microsoft', 'It', 'Idris Elba Walks','Bryan Johnson Wants','Mark Cuban‚Äôs','Bill Gates Does'\n",
    "remove_guests = ['Dan Corder and Eugene', 'Anele and Sizwe', 'Microsoft', 'It', 'Idris Elba Walks','Bryan Johnson Wants','Mark Cuban‚Äôs','Bill Gates Does']\n",
    "df_matches_filtered_loaded = df_matches_filtered_loaded[~df_matches_filtered_loaded['guest'].isin(remove_guests)]\n",
    "print(len(df_matches_filtered_loaded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5525b679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "show_id                 show_name                                      \n",
       "7i3PwGmUHVkzQam9ARdcfo  What Now? with Trevor Noah | SiriusXM Podcasts+    45\n",
       "53dHyhzazFrmPhwuambyuM  The Daily Show: Ears Edition                       25\n",
       "1VXcH8QHkjRcTCEd88U3ti  TED Talks Daily                                    17\n",
       "6kAsbP8pxwaU2kPibKTuHE  Armchair Expert with Dax Shepard                   15\n",
       "3sLR8M7JchYmhWuf8QtlyO  Amanpour                                           15\n",
       "4ZTHlQzCm7ipnRn1ypnl1Z  The New Yorker Radio Hour                          13\n",
       "42ntT7XnfdbbYetUQu0vu5  On with Kara Swisher                               12\n",
       "5RVoEOIACQbBDZnGBJ7Ec2  The Jordan Harbinger Show                          12\n",
       "3jcdWsWV2KBwsBvBlLCAGx  The Breakfast Club                                 12\n",
       "2qoc5EIZbcuGUgQhl3QeYd  Talk Easy with Sam Fragoso                         12\n",
       "3ZK78RyjzfJLWJ41fAaolZ  The Late Show Pod Show with Stephen Colbert        12\n",
       "4eylg9GZJOVvUhTynt4jjA  Worklife with Adam Grant                           12\n",
       "47jQcyRcrM1EoV0sU39N9F  Decoder with Nilay Patel                           11\n",
       "6NOJ6IkTb2GWMj1RpmtnxP  The Gray Area with Sean Illing                     11\n",
       "5qSUyCrk9KR69lEiXbjwXM  The Tim Ferriss Show                               11\n",
       "6VOaWm3AulzIjkk7TKZQIY  Armchair Expert with Dax Shepard | Wondery+        11\n",
       "5Ag8JmOYQv1Ue4ko93hZUp  City Arts & Lectures                               11\n",
       "7nqJajzyZo3gdisGk7XhQx  Real Time with Bill Maher                          11\n",
       "5EqqB52m2bsr4k1Ii7sStc  On Purpose with Jay Shetty                         11\n",
       "3dpIUpUortdZ91gi08QO8Z  NPR's Book of the Day                              10\n",
       "Name: guest, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_matches_filtered_loaded.copy()\n",
    "\n",
    "# 0) hygiene\n",
    "df = df.dropna(subset=[\"guest\", \"show_id\"])\n",
    "\n",
    "# (optional) exclude Trevor‚Äôs own podcast so you only see ‚Äúother‚Äù shows\n",
    "TREVOR_SHOW_ID = \"122imavATqSE7eCyXIcqZL\"\n",
    "df = df[df[\"show_id\"] != TREVOR_SHOW_ID]\n",
    "\n",
    "# 1) dedupe to one row per guest‚Üîshow (avoid inflating counts when a guest did multiple episodes)\n",
    "gs_unique = df.drop_duplicates(subset=[\"guest\", \"show_id\"])\n",
    "\n",
    "# 2) find shows with ‚â• 2 distinct guests\n",
    "shows_multi = (\n",
    "    gs_unique.groupby(\"show_id\")[\"guest\"]\n",
    "    .nunique()\n",
    "    .loc[lambda s: s >= 2]\n",
    "    .index\n",
    ")\n",
    "\n",
    "# 3) filter the original df down to only those shows\n",
    "df_subset = df[df[\"show_id\"].isin(shows_multi)].copy()\n",
    "\n",
    "# (optional) if you want exactly one row per guest‚Üîshow pair in the result:\n",
    "df_subset_unique = df_subset.drop_duplicates(subset=[\"guest\", \"show_id\"])\n",
    "\n",
    "# (optional) quick summary: how many distinct guests per show\n",
    "summary = (\n",
    "    df_subset_unique.groupby([\"show_id\", \"show_name\"])[\"guest\"]\n",
    "    .nunique()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "summary.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dde8a4",
   "metadata": {},
   "source": [
    "## Exporting guests and podcasts as nodes and edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3e2b22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Wrote data/nodes.csv (nodes=1455)\n",
      "‚úÖ Wrote data/links.csv (links=4038)\n"
     ]
    }
   ],
   "source": [
    "# --- assumptions ---\n",
    "# df_matches exists with at least: guest, show_id, show_name, episode_id\n",
    "# optional (used if present): episode_url, show_publisher\n",
    "\n",
    "# 1) Clean + guard\n",
    "required = {\"guest\",\"show_id\",\"show_name\",\"episode_id\"}\n",
    "missing = required - set(df_subset.columns)\n",
    "if missing:\n",
    "    raise RuntimeError(f\"df_subset is missing required columns: {missing}\")\n",
    "\n",
    "df = df_subset.dropna(subset=[\"guest\",\"show_id\",\"show_name\"]).copy()\n",
    "for col in [\"guest\",\"show_id\",\"show_name\"]:\n",
    "    df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "# 2) Build NODES\n",
    "guest_nodes = (\n",
    "    df[[\"guest\"]]\n",
    "    .drop_duplicates()\n",
    "    .assign(id=lambda d: \"guest:\" + d[\"guest\"],\n",
    "            label=lambda d: d[\"guest\"],\n",
    "            group=\"guest\",\n",
    "            url=\"\",\n",
    "            publisher=\"\")\n",
    ")[[\"id\",\"label\",\"group\",\"url\",\"publisher\"]]\n",
    "\n",
    "show_nodes = (\n",
    "    df[[\"show_id\",\"show_name\"] + ([\"show_publisher\"] if \"show_publisher\" in df.columns else [])]\n",
    "    .drop_duplicates()\n",
    "    .rename(columns={\"show_id\":\"id\",\"show_name\":\"label\",\n",
    "                     (\"show_publisher\" if \"show_publisher\" in df.columns else \"show_publisher\"): \"publisher\"})\n",
    ")\n",
    "# attach a representative URL per show if available\n",
    "if \"episode_url\" in df.columns:\n",
    "    url_map = df.groupby(\"show_id\")[\"episode_url\"].first().to_dict()\n",
    "    show_nodes[\"url\"] = show_nodes[\"id\"].map(url_map).fillna(\"\")\n",
    "else:\n",
    "    show_nodes[\"url\"] = \"\"\n",
    "\n",
    "# ensure publisher column exists\n",
    "if \"publisher\" not in show_nodes.columns:\n",
    "    show_nodes[\"publisher\"] = \"\"\n",
    "\n",
    "show_nodes[\"group\"] = \"podcast\"\n",
    "show_nodes = show_nodes[[\"id\",\"label\",\"group\",\"url\",\"publisher\"]]\n",
    "\n",
    "nodes = pd.concat([guest_nodes, show_nodes], ignore_index=True)\n",
    "nodes = nodes.fillna(\"\")\n",
    "\n",
    "# 3) Build LINKS (guest‚Üîshow edges; weight = episode count)\n",
    "links = (\n",
    "    df.groupby([\"guest\",\"show_id\"], dropna=False)\n",
    "      .agg(value=(\"episode_id\",\"count\"))\n",
    "      .reset_index()\n",
    ")\n",
    "links[\"source\"] = \"guest:\" + links[\"guest\"]\n",
    "links[\"target\"] = links[\"show_id\"]\n",
    "links = links[[\"source\",\"target\",\"value\"]].astype({\"value\":\"int64\"})\n",
    "\n",
    "# 4) Write to CSV\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "nodes_path = \"data/nodes.csv\"\n",
    "links_path = \"data/links.csv\"\n",
    "nodes.to_csv(nodes_path, index=False)\n",
    "links.to_csv(links_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Wrote {nodes_path} (nodes={len(nodes)})\")\n",
    "print(f\"‚úÖ Wrote {links_path} (links={len(links)})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
